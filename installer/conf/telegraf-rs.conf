# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply prepend
# them with $. For strings the variable must be within quotes (ie, "$STR_VAR"),
# for numbers and booleans they should be plain (ie, $INT_VAR, $BOOL_VAR)


# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"
  # cluster = "$ACS_RESOURCE_NAME"
  #node = $NODE_IP
  AgentVersion = "$AGENT_VERSION"
  AKS_RESOURCE_ID = "$AKS_RESOURCE_ID"
  Region = "$AKS_REGION"
  ClusterName = "$AKS_CLUSTER_NAME"
  ClusterType = "AKS"
  Computer = "placeholder_hostname"
  ControllerType = "$CONTROLLER_TYPE"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "60s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
  ## output, and will flush this buffer on a successful write. Oldest metrics
  ## are dropped first when this buffer fills.
  ## This buffer only fills when writes fail to output plugin(s).
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. You shouldn't set this below
  ## interval. Maximum flush_interval will be flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Logging configuration:
  ## Run telegraf with debug log messages.
  debug = false
  ## Run telegraf in quiet mode (error log messages only).
  quiet = true
  ## Specify the log file name. The empty string means to log to stderr.
  logfile = "/var/opt/microsoft/docker-cimprov/log/telegraf.log"

  ## Override default hostname, if empty use os.Hostname()
  #hostname = "placeholder_hostname"
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = true


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

# Send aggregate metrics to Azure Monitor
[[outputs.azure_monitor]]
  ## Timeout for HTTP writes.
  # timeout = "20s"

  ## Set the namespace prefix, defaults to "Telegraf/<input-name>".
  namespace_prefix = "Insights.Container/"

  ## Azure Monitor doesn't have a string value type, so convert string
  ## fields to dimensions (a.k.a. tags) if enabled. Azure Monitor allows
  ## a maximum of 10 dimensions so Telegraf will only send the first 10
  ## alphanumeric dimensions.
  strings_as_dimensions = true

  ## Both region and resource_id must be set or be available via the
  ## Instance Metadata service on Azure Virtual Machines.
  #
  ## Azure Region to publish metrics against.
  ##   ex: region = "southcentralus"
  region = "placeholder_region"
  #
  ## The Azure Resource ID against which metric will be logged, e.g.
  #resource_id = "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>"
  resource_id = "placeholder_resource_id"

  #azure_tenant_id = "placeholder_azure_tenant_id"

  #azure_client_id = "placeholder_azure_client_id"

  #azure_client_secret = "placeholder_azure_client_secret"

  #namepass = ["nodes", "pods", "containers","prometheus"]
  namedrop = ["filestat", "telegraf_telemetry"]
  tagdrop = ["AgentVersion","AKS_RESOURCE_ID","Region","ClusterName","ClusterType", "Computer", "ControllerType"]

[[outputs.application_insights]]
  ## Instrumentation key of the Application Insights resource.
  instrumentation_key = "$APPLICATIONINSIGHTS_KEY"

  ## Timeout for closing (default: 5s).
  # timeout = "5s"

  ## Enable additional diagnostic logging.
  # enable_diagnostic_logging = false

  ## Context Tag Sources add Application Insights context tags to a tag value.
  ##
  ## For list of allowed context tag keys see:
  ## https://github.com/Microsoft/ApplicationInsights-Go/blob/master/appinsights/contracts/contexttagkeys.go
  # [outputs.application_insights.context_tag_sources]
  #   "ai.cloud.role" = "kubernetes_container_name"
  #   "ai.cloud.roleInstance" = "kubernetes_pod_name"
  namepass = ["filestat", "telegraf_telemetry"]

###############################################################################
#                            PROCESSOR PLUGINS                                #
###############################################################################

# # Convert values to another metric value type
# [[processors.converter]]
#   ## Tags to convert
#   ##
#   ## The table key determines the target type, and the array of key-values
#   ## select the keys to convert.  The array may contain globs.
#   ##   <target-type> = [<tag-key>...]
#   [processors.converter.tags]
#     string = ["device"]
#     integer = []
#     unsigned = []
#     boolean = []
#     float = []
#
#   ## Fields to convert
#   ##
#   ## The table key determines the target type, and the array of key-values
#   ## select the keys to convert.  The array may contain globs.
#   ##   <target-type> = [<field-key>...]
#   [processors.converter.fields]
#     tag = ["host"]
#     string = []
#     integer = []
#     unsigned = []
#     boolean = []
#     float = []


# # Map enum values according to given table.
# [[processors.enum]]
#   [[processors.enum.mapping]]
#     ## Name of the field to map
#     field = "status"
#
#     ## Destination field to be used for the mapped value.  By default the source
#     ## field is used, overwriting the original value.
#     # dest = "status_code"
#
#     ## Default value to be used for all values not contained in the mapping
#     ## table.  When unset, the unmodified value for the field will be used if no
#     ## match is found.
#     # default = 0
#
#     ## Table of mappings
#     [processors.enum.mapping.value_mappings]
#       green = 1
#       yellow = 2
#       red = 3


# # Apply metric modifications using override semantics.
# [[processors.override]]
#   ## All modifications on inputs and aggregators can be overridden:
#   # name_override = "new_name"
#   # name_prefix = "new_name_prefix"
#   # name_suffix = "new_name_suffix"
#
#   ## Tags to be added (all values must be strings)
#   # [processors.override.tags]
#   #   additional_tag = "tag_value"


# # Parse a value in a specified field/tag(s) and add the result in a new metric
# [[processors.parser]]
#   ## The name of the fields whose value will be parsed.
#   parse_fields = []
#
#   ## If true, incoming metrics are not emitted.
#   drop_original = false
#
#   ## If set to override, emitted metrics will be merged by overriding the
#   ## original metric using the newly parsed metrics.
#   merge = "override"
#
#   ## The dataformat to be read from files
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"


# # Print all metrics that pass through this filter.
# [[processors.printer]]


# # Transforms tag and field values with regex pattern
# [[processors.regex]]
#   ## Tag and field conversions defined in a separate sub-tables
#   # [[processors.regex.tags]]
#   #   ## Tag to change
#   #   key = "resp_code"
#   #   ## Regular expression to match on a tag value
#   #   pattern = "^(\\d)\\d\\d$"
#   #   ## Pattern for constructing a new value (${1} represents first subgroup)
#   #   replacement = "${1}xx"
#
#   # [[processors.regex.fields]]
#   #   key = "request"
#   #   ## All the power of the Go regular expressions available here
#   #   ## For example, named subgroups
#   #   pattern = "^/api(?P<method>/[\\w/]+)\\S*"
#   #   replacement = "${method}"
#   #   ## If result_key is present, a new field will be created
#   #   ## instead of changing existing field
#   #   result_key = "method"
#
#   ## Multiple conversions may be applied for one field sequentially
#   ## Let's extract one more value
#   # [[processors.regex.fields]]
#   #   key = "request"
#   #   pattern = ".*category=(\\w+).*"
#   #   replacement = "${1}"
#   #   result_key = "search_category"


# # Rename measurements, tags, and fields that pass through this filter.
# [[processors.rename]]


# # Perform string processing on tags, fields, and measurements
[[processors.rename]]
  [[processors.rename.replace]]
     measurement = "kubernetes_daemonset"
     dest = "daemonsets"
  [[processors.rename.replace]]
     measurement = "kubernetes_deployment"
     dest = "deployments"
  [[processors.rename.replace]]
     measurement = "kubernetes_deployment"
     dest = "statefulsets"
  [[processors.rename.replace]]
     measurement = "kubernetes_node"
     dest = "nodes"
  [[processors.rename.replace]]
     measurement = "kubernetes_pod_container"
     dest = "containers"
  [[processors.rename.replace]]
     field = "current_number_scheduled"
     dest = "currentNumberScheduled"
  [[processors.rename.replace]]
     field = "desired_number_scheduled"
     dest = "desiredNumberScheduled"
  [[processors.rename.replace]]
     field = "number_available"
     dest = "numberAvailable"
  [[processors.rename.replace]]
     field = "number_unavailable"
     dest = "numUnavailable"
  [[processors.rename.replace]]
     field = "number_ready"
     dest = "numReady"
  [[processors.rename.replace]]
     field = "replicas_available"
     dest = "numReplicasAvailable"
  [[processors.rename.replace]]
     field = "replicas_unavailable"
     dest = "numReplicasUnavailable"
  [[processors.rename.replace]]
     field = "capacity_cpu_cores"
     dest = "capacityCpuCores"
  [[processors.rename.replace]]
     field = "capacity_memory_bytes"
     dest = "capacityMemoryBytes"
  [[processors.rename.replace]]
     field = "capacity_pods"
     dest = "capacityNumPods"
  [[processors.rename.replace]]
     field = "allocatable_pods"
     dest = "allocatableNumPods"
  [[processors.rename.replace]]
     field = "allocatable_cpu_cores"
     dest = "allocatableCpuCores"
  [[processors.rename.replace]]
     field = "allocatable_memory_bytes"
     dest = "allocatableMemoryBytes"
  [[processors.rename.replace]]
     field = "restarts_total"
     dest = "restartsTotal"
  [[processors.rename.replace]]
     field = "resource_requests_cpu_units"
     dest = "resourceRequestsCpuUnits"
  [[processors.rename.replace]]
     field = "resource_requests_memory_bytes"
     dest = "resourceRequestsMemoryBytes"
  [[processors.rename.replace]]
     field = "resource_limits_cpu_units"
     dest = "resourceLimitsCpuUnits"
  [[processors.rename.replace]]
     field = "resource_limits_memory_bytes"
     dest = "resourceLimitsMemoryBytes"
  [[processors.rename.replace]]
     field = "spec_replicas"
     dest = "numSpecReplicas"
  [[processors.rename.replace]]
     field = "replicas_current"
     dest = "numCurrentReplicas"
  [[processors.rename.replace]]
     field = "replicas_ready"
     dest = "numReadyReplicas"
  [[processors.rename.replace]]
     tag = "daemonset_name"
     dest = "daemonsetName"
  [[processors.rename.replace]]
     tag = "deployment_name"
     dest = "deploymentName"
  [[processors.rename.replace]]
     tag = "container_name"
     dest = "containerName"
  [[processors.rename.replace]]
     tag = "statefulset_name"
     dest = "statefulsetName"
  [[processors.rename.replace]]
     tag = "node_name"
     dest = "nodeName"

#   ## Convert a tag value to uppercase
#   # [[processors.strings.uppercase]]
#   #   tag = "method"
#
#   ## Convert a field value to lowercase and store in a new field
#   # [[processors.strings.lowercase]]
#   #   field = "uri_stem"
#   #   dest = "uri_stem_normalised"
#
#   ## Trim leading and trailing whitespace using the default cutset
#   # [[processors.strings.trim]]
#   #   field = "message"
#
#   ## Trim leading characters in cutset
#   # [[processors.strings.trim_left]]
#   #   field = "message"
#   #   cutset = "\t"
#
#   ## Trim trailing characters in cutset
#   # [[processors.strings.trim_right]]
#   #   field = "message"
#   #   cutset = "\r\n"
#
#   ## Trim the given prefix from the field
#   # [[processors.strings.trim_prefix]]
#   #   field = "my_value"
#   #   prefix = "my_"
#
#   ## Trim the given suffix from the field
#   # [[processors.strings.trim_suffix]]
#   #   field = "read_count"
#   #   suffix = "_count"


# # Print all metrics that pass through this filter.
# [[processors.topk]]
#   ## How many seconds between aggregations
#   # period = 10
#
#   ## How many top metrics to return
#   # k = 10
#
#   ## Over which tags should the aggregation be done. Globs can be specified, in
#   ## which case any tag matching the glob will aggregated over. If set to an
#   ## empty list is no aggregation over tags is done
#   # group_by = ['*']
#
#   ## Over which fields are the top k are calculated
#   # fields = ["value"]
#
#   ## What aggregation to use. Options: sum, mean, min, max
#   # aggregation = "mean"
#
#   ## Instead of the top k largest metrics, return the bottom k lowest metrics
#   # bottomk = false
#
#   ## The plugin assigns each metric a GroupBy tag generated from its name and
#   ## tags. If this setting is different than "" the plugin will add a
#   ## tag (which name will be the value of this setting) to each metric with
#   ## the value of the calculated GroupBy tag. Useful for debugging
#   # add_groupby_tag = ""
#
#   ## These settings provide a way to know the position of each metric in
#   ## the top k. The 'add_rank_field' setting allows to specify for which
#   ## fields the position is required. If the list is non empty, then a field
#   ## will be added to each and every metric for each string present in this
#   ## setting. This field will contain the ranking of the group that
#   ## the metric belonged to when aggregated over that field.
#   ## The name of the field will be set to the name of the aggregation field,
#   ## suffixed with the string '_topk_rank'
#   # add_rank_fields = []
#
#   ## These settings provide a way to know what values the plugin is generating
#   ## when aggregating metrics. The 'add_agregate_field' setting allows to
#   ## specify for which fields the final aggregation value is required. If the
#   ## list is non empty, then a field will be added to each every metric for
#   ## each field present in this setting. This field will contain
#   ## the computed aggregation for the group that the metric belonged to when
#   ## aggregated over that field.
#   ## The name of the field will be set to the name of the aggregation field,
#   ## suffixed with the string '_topk_aggregate'
#   # add_aggregate_fields = []



###############################################################################
#                            AGGREGATOR PLUGINS                               #
###############################################################################

# # Keep the aggregate basicstats of each metric passing through.
# [[aggregators.basicstats]]
#   ## General Aggregator Arguments:
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false


# # Create aggregate histograms.
# [[aggregators.histogram]]
#   ## The period in which to flush the aggregator.
#   period = "30s"
#
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false
#
#   ## Example config that aggregates all fields of the metric.
#   # [[aggregators.histogram.config]]
#   #   ## The set of buckets.
#   #   buckets = [0.0, 15.6, 34.5, 49.1, 71.5, 80.5, 94.5, 100.0]
#   #   ## The name of metric.
#   #   measurement_name = "cpu"
#
#   ## Example config that aggregates only specific fields of the metric.
#   # [[aggregators.histogram.config]]
#   #   ## The set of buckets.
#   #   buckets = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]
#   #   ## The name of metric.
#   #   measurement_name = "diskio"
#   #   ## The concrete fields of metric
#   #   fields = ["io_time", "read_time", "write_time"]


# # Keep the aggregate min/max of each metric passing through.
# [[aggregators.minmax]]
#   ## General Aggregator Arguments:
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false


# # Count the occurance of values in fields.
# [[aggregators.valuecounter]]
#   ## General Aggregator Arguments:
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false
#   ## The fields for which the values will be counted
#   fields = []



###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

# Read metrics about cpu usage
#[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
#  percpu = false
  ## Whether to report total system cpu stats or not
#  totalcpu = true
  ## If true, collect raw CPU time metrics.
#  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states.
#  report_active = true
#  fieldpass = ["usage_active","cluster","node","host","device"]
#  taginclude = ["cluster","cpu","node"]
  


  # Read metrics from one or many prometheus clients
#[[inputs.prometheus]]
  ## An array of urls to scrape metrics from.
#  urls = ["https://$METRICS_SERVER_SERVICE_HOST/metrics"]

  ## An array of Kubernetes services to scrape metrics from.
  # kubernetes_services = ["http://my-service-dns.my-namespace:9100/metrics"]

  ## Kubernetes config file to create client from.
  # kube_config = "/path/to/kubernetes.config"

  ## Scrape Kubernetes pods for the following prometheus annotations:
  ## - prometheus.io/scrape: Enable scraping for this pod
  ## - prometheus.io/scheme: If the metrics endpoint is secured then you will need to
  ##     set this to 'https' & most likely set the tls config.
  ## - prometheus.io/path: If the metrics path is not /metrics, define it with this annotation.
  ## - prometheus.io/port: If port is not 9102 use this annotation
  # monitor_kubernetes_pods = true

  ## Use bearer token for authorization
#  bearer_token = "/var/run/secrets/kubernetes.io/serviceaccount/token"

  ## Specify timeout duration for slower prometheus clients (default is 3s)
#  response_timeout = "15s"

  ## Optional TLS Config
#  tls_ca = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
  # tls_cert = /path/to/certfile
  # tls_key = /path/to/keyfile
  ## Use TLS but skip chain & host verification
#  insecure_skip_verify = true
  # Read stats about given file(s)
[[inputs.filestat]]
  ## Files to gather stats about.
  interval = "15m"
  ## These accept standard unix glob matching rules, but with the addition of
  ## ** as a "super asterisk". See https://github.com/gobwas/glob.
  files = ["/var/opt/microsoft/docker-cimprov/log/telegraf.log"]
  ## If true, read the entire file and calculate an md5 checksum.
  md5 = false
[[inputs.kube_inventory]]
  ## URL for the Kubernetes API
  #url = "https://127.0.0.1"
  url = "$K8SSERVICEHOST"

  ## Namespace to use
  # namespace = "default"

  ## Use bearer token for authorization. ('bearer_token' takes priority)
  bearer_token = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  ## OR
  # bearer_token_string = "abc_123"

  ## Set response_timeout (default 5 seconds)
  response_timeout = "15s"

  ## Optional Resources to exclude from gathering
  ## Leave them with blank with try to gather everything available.
  ## Values can be - "daemonsets", deployments", "nodes", "persistentvolumes",
  ## "persistentvolumeclaims", "pods", "statefulsets"
  # resource_exclude = [ "deployments", "nodes", "statefulsets" ]

  ## Optional Resources to include when gathering
  ## Overrides resource_exclude if both set.
  # resource_include = [ "deployments", "nodes", "statefulsets" ]

  ## Optional TLS Config
  tls_ca = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
  # tls_cert = "/path/to/certfile"
  # tls_key = "/path/to/keyfile"
  ## Use TLS but skip chain & host verification
  insecure_skip_verify = true

  namepass = ["kubernetes_daemonset", "kubernetes_deployment", "kubernetes_node", "kubernetes_pod_container", "kubernetes_statefulset"]
  fieldpass = ["current_number_scheduled", "desired_number_scheduled", "number_available", "number_unavailable", "number_ready", "replicas_available", "replicas_unavailable", "capacity_cpu_cores", "capacity_memory_bytes", "capacity_pods", "allocatable_pods", "allocatable_cpu_cores", "allocatable_memory_bytes", "restarts_total","resource_requests_cpu_units", "resource_requests_memory_bytes", "resource_limits_cpu_units", "resource_limits_memory_bytes" , "spec_replicas", "replicas_current", "replicas_ready"]
  taginclude = ["nodeName", "daemonset_name", "namespace", "deployment_name", "container_name", "namespace", "node_name","statefulset_name"]
[[inputs.exec]]
  ## Commands array
  interval = "15m"
  commands = [
    "/opt/microsoft/docker-cimprov/bin/Telegraf403Telemetry.sh"
  ]

  ## Timeout for each command to complete.
  timeout = "15s"

  ## measurement name suffix (for separating different commands)
  name_suffix = "_telemetry"

  ## Data format to consume.
  ## Each data format has its own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
  data_format = "influx"